{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5138926",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "- Input Layer: 2 neurons (Let's denote them as x1 and x2).\n",
    "- Hidden Layer: 2 neurons (Let's denote the activation of these neurons as h1 and h2).\n",
    "- Output Layer: 1 neuron (Let's denote its activation as o1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ba9fd",
   "metadata": {},
   "source": [
    "## 1. Weights and Biases Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142cc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a11cb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# tensor = tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e50add",
   "metadata": {},
   "source": [
    "Let's denote the weights from the input layer to the hidden layer as W₁, and the weights from the hidden layer to the output layer as W₂. The biases are denoted as b₁ for the hidden layer and b₂ for the output layer.\n",
    "\n",
    "- W₁: A matrix where each element W₁ᵢⱼ represents the weight from input neuron i to hidden neuron j. Since there are 2 input neurons and 2 hidden neurons, W₁ will be a 2x2 matrix.\n",
    "- b₁: A vector of biases for the hidden layer, so it will have 2 elements.\n",
    "- W₂: A matrix for weights from the hidden layer to the output layer. Here, since there are 2 hidden neurons and 1 output neuron, W₂ will be a 2x1 matrix.\n",
    "- b₂: The bias for the output neuron, a scalar since there is only one output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "040bad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # For reproducibility\n",
    "\n",
    "# Define the weights and biases with real numbers\n",
    "W1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)  # 2x2 matrix for input to hidden layer\n",
    "W1_bef = W1.clone()\n",
    "b1 = torch.tensor([0.1, 0.2], requires_grad=True)              # 2 elements vector for hidden layer biases\n",
    "b1_bef = b1.clone()\n",
    "W2 = torch.tensor([[0.5], [0.6]], requires_grad=True)           # 2x1 matrix for hidden to output layer\n",
    "W2_bef = W2.clone()\n",
    "b2 = torch.tensor([0.3], requires_grad=True)                    # Scalar for output layer bias\n",
    "b2_bef = b2.clone()\n",
    "\n",
    "# Define the input\n",
    "X = torch.tensor([1.0, 2.0], requires_grad=False)  # 2 elements vector for input      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cdbbd2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000],\n",
       "        [0.3000, 0.4000]], requires_grad=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eaadd0",
   "metadata": {},
   "source": [
    "## 2. Forward Pass\n",
    "\n",
    "Let's denote the input vector as X = [x1, x2].\n",
    "\n",
    "From Input to Hidden Layer\n",
    "Linear Transformation: The inputs are multiplied by the weights and added to the biases to compute the input to the hidden layer neurons.\n",
    "\n",
    "Z₁ = XW₁ + b₁\n",
    "\n",
    "Here, Z₁ is a 1x2 vector (since there are two neurons in the hidden layer).\n",
    "\n",
    "Activation Function: An activation function (let's use ReLU for simplicity, which is defined as f(x) = max(0, x)) is applied element-wise to Z₁ to obtain the activations of the hidden layer:\n",
    "\n",
    "H=f(Z₁)\n",
    "\n",
    "Here, H is also a 1x2 vector.\n",
    "\n",
    "From Hidden to Output Layer\n",
    "Linear Transformation: Similar to the previous layer, we compute the input to the output neuron:\n",
    "\n",
    "Z₂ = HW₂ + b₂\n",
    "\n",
    "Z₂ is a scalar since there is only one output neuron.\n",
    "\n",
    "Activation Function: If this were a binary classification problem, we might use the sigmoid activation function here. But let's keep it simple and assume it's a regression problem, so no activation function is applied, meaning the output is just Z₂.\n",
    "\n",
    "Output\n",
    "The final output of the network, O, for the input vector X is just Z₂ after the second linear transformation (since we're not applying another activation function for this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e5fd642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network: 1.4200000762939453\n"
     ]
    }
   ],
   "source": [
    "# Forward pass from input to hidden layer\n",
    "Z1 = torch.matmul(X, W1) + b1  # Linear transformation\n",
    "a1 = F.relu(Z1)  # Activation function (ReLU)\n",
    "\n",
    "# Forward pass from hidden to output layer\n",
    "Z2 = torch.matmul(a1, W2) + b2  # Linear transformation\n",
    "a2 = Z2  # Identity function for the output\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of the network:\", a2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cb454",
   "metadata": {},
   "source": [
    "Matmul : Under the hood, if you multiply a matrix A by a matrix B, where A has dimensions m×n and B has dimensions n×p, the resulting matrix C will have dimensions m×p. Each element of C is computed as the dot product of the rows of A with the columns of B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49f655",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff49f7a",
   "metadata": {},
   "source": [
    "Method used to calculate the gradient of the loss function with respect to each weight in the network, by applying the chain rule repeatedly from the output layer back towards the input layer. This gradient information is then used to update the weights in a way that minimizes the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86115e13",
   "metadata": {},
   "source": [
    "**Conceptual Foundations:**\n",
    "\n",
    "*   **Gradient:** This represents the direction and rate at which a function increases most rapidly. For our purposes, we actually want to move in the opposite direction to minimize our loss function, which means we're looking for the path that decreases the function most rapidly.\n",
    "    \n",
    "*   **Chain Rule of Calculus:** This rule allows us to compute the derivative of composite functions. To put it simply, if you have a function h(x)\\=g(f(x)), then the derivative h′(x) (which represents how h changes as x changes) is given by g′(f(x))⋅f′(x). In simpler terms, this means the rate of change of h with respect to x is the product of the rate of change of g with respect to f and the rate of change of f with respect to x. This is useful for understanding how changes in x affect the output of the function h."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1913d",
   "metadata": {},
   "source": [
    "For simplicity, let's assume we're using **mean squared error (MSE)** as the loss function and the identity function as the activation function for the output layer, while using ReLU for the hidden layer as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "771844be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume some target value y (the true label)\n",
    "y = torch.tensor([0.7])  # For example purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed8c5f",
   "metadata": {},
   "source": [
    "#### To understand the effect of W2[0]=0.5 on the loss, we perform the backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5b1b282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.6000]], requires_grad=True)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae65ea4",
   "metadata": {},
   "source": [
    "### 1. Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5860567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.2592000663280487\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = 0.5 * (a2 - y) ** 2\n",
    "print(f\"Initial loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e21a14",
   "metadata": {},
   "source": [
    "### 2. Compute the Gradient of the Loss with Respect to Output:\n",
    "\n",
    "The derivative of the loss function with respect to the output of the network (o) is:\n",
    "\n",
    "Since a2 = Z2 <break>\n",
    "   \n",
    "∂L/∂a2 = a2−y\n",
    "\n",
    "This will give us how much the loss changes with respect to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ddbca83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7200], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "353cb37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.0 a_{2} - 1.0 y$"
      ],
      "text/plain": [
       "1.0*a2 - 1.0*y"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the variables\n",
    "a2_, y_ = sp.symbols('a2 y')\n",
    "\n",
    "# Define the function\n",
    "f = 0.5 * (a2_ - y_) ** 2\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "derivative_f = sp.diff(f, a2_)\n",
    "derivative_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636b546",
   "metadata": {},
   "source": [
    "### 3. Derivative of Output (Z2) w.r.t. W2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5bf54442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass from hidden to output layer\n",
    "# Z2 = torch.matmul(a1, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25073af0",
   "metadata": {},
   "source": [
    "∂Z2/∂A1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382db95",
   "metadata": {},
   "source": [
    "Z2 depends linearly on W2[0] through the first term of a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "87e52699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a_{11}$"
      ],
      "text/plain": [
       "a11"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z2 = a2\n",
    "a11_, w21_, b2_ = sp.symbols('a11 w21 b2')\n",
    "\n",
    "# Define the function\n",
    "z2_ = (a11_ * w21_) + b2_\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "derivative_f = sp.diff(z2_, w21_)\n",
    "derivative_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8761c3",
   "metadata": {},
   "source": [
    "### 4. Chain Rule for W2[0] \n",
    "\n",
    "Combine the derivatives using the chain rule to find how Loss changes w.r.t W2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9fb75",
   "metadata": {},
   "source": [
    "∂L/∂W2[0] = ∂L/∂a2 * ∂a2/Z2 * ∂Z2/∂aW2[0] = (a2−y) * 1 * First element of A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "25cd30bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7200], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "798d2812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ae2a9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2592], grad_fn=<MulBackward0>)\n",
      "tensor([1.4200], grad_fn=<AddBackward0>)\n",
      "tensor([0.7200], grad_fn=<SubBackward0>)\n",
      "tensor([0.7000])\n",
      "\n",
      "tensor([0.5760], grad_fn=<MulBackward0>): \n",
      "This value indicates that if W2[0] is increased by a small amount,\n",
      "the loss L is expected to also increase (since the gradient is positive),\n",
      "suggesting the model would be getting worse.\n"
     ]
    }
   ],
   "source": [
    "# Backward pass calculations for W2[0]\n",
    "# dL/da2\n",
    "dL_da2 = a2 - y # output - target\n",
    "\n",
    "# Since a2 = Z2 directly and dZ2/dW2[0] = first element of A1 (as a2 = Z2 = A1 * W2 + b2)\n",
    "dZ2_dw2_1 = a1[0] # first element of calculated values after first activation function in the hidden layer\n",
    "\n",
    "# Chain rule to get dL/dw2_1\n",
    "dL_dw2_1 = dL_da2 * dZ2_dw2_1\n",
    "\n",
    "# Values for illustration\n",
    "print(loss)\n",
    "print(a2)\n",
    "print(dL_da2)\n",
    "print(y)\n",
    "print()\n",
    "print(f\"\"\"{dL_dw2_1}: \n",
    "This value indicates that if W2[0] is increased by a small amount,\n",
    "the loss L is expected to also increase (since the gradient is positive),\n",
    "suggesting the model would be getting worse.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f8c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6db6d19b",
   "metadata": {},
   "source": [
    "#### To understand the effect of W1[0][1]= 0.2 on the loss, we perform the backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30c198",
   "metadata": {},
   "source": [
    "1. Gradient of the Loss with Respect to Output.\n",
    "2. Derivative of Output (Z2) w.r.t. A12 (Hidden Layer Activation) = W2[1] \n",
    "3. Derivative of A12 w.r.t. activation function ReLU.\n",
    "3. Derivative of Output (Z12) w.r.t. W1[0][1] = X1\n",
    "3. Chain Rule for W1[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0d8bec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass from hidden to output layer\n",
    "# Z2 = torch.matmul(a1, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb5d49",
   "metadata": {},
   "source": [
    "Z2 depends linearly on W2[1] through the second term of a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "35006006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle w_{22}$"
      ],
      "text/plain": [
       "w22"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivative of Output (Z2) w.r.t. A12 (Hidden Layer Activation) = W2[1] (W22_)\n",
    "# z2 = a2\n",
    "a12_, w22_, b2_ = sp.symbols('a12 w22 b2')\n",
    "\n",
    "# Define the function\n",
    "z2_ = (a12_ * w22_) + b2_\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "derivative_f = sp.diff(z2_, a12_)\n",
    "derivative_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a5d96390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x_{1}$"
      ],
      "text/plain": [
       "x1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivative of Output (Z12) w.r.t. W1[0][1] (W1)\n",
    "# z2 = a2\n",
    "x1_, w12_, b11_ = sp.symbols('x1 w12 b11')\n",
    "\n",
    "# Define the function\n",
    "z12_ = (x1_ * w12_) + b11_\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "derivative_f = sp.diff(z12_, w12_)\n",
    "derivative_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "71afbff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7200], grad_fn=<SubBackward0>)\n",
      "tensor([0.6000], grad_fn=<SelectBackward0>)\n",
      "tensor(1.2000, grad_fn=<ReluBackward0>)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(a2 - y)\n",
    "print(W2[1])\n",
    "print(F.relu(Z1[1]))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "af0bbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass calculations for W1[0][1] (w1_2)\n",
    "# dL/da2\n",
    "dL_da2 = a2 - y # output - target\n",
    "\n",
    "# Since a2 = Z2 directly and dZ2/da1_2 = W2[1]\n",
    "# represents the derivative of the output Z2 w.r.t the activation A12 of the second neuron in the hidden layer.\n",
    "dZ2_da1_2 = W2[1] \n",
    "\n",
    "# derivative of the ReLU activation function applied to the pre-activation Z1[1] of the second neuron.\n",
    "da1_2_d_z1_2 = F.relu(Z1[1])\n",
    "\n",
    "# derivative of the pre-activation Z1[1] with respect to the weight W1[1]\n",
    "d_z1_2_d_w1_2 = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bab5430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule to get dL/dw1_2\n",
    "dL_dw1_2 = dL_da2 * dZ2_da1_2 * da1_2_d_z1_2 * d_z1_2_d_w1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b6fd4ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2592], grad_fn=<MulBackward0>)\n",
      "tensor([1.4200], grad_fn=<AddBackward0>)\n",
      "tensor([0.7200], grad_fn=<SubBackward0>)\n",
      "tensor([0.7000])\n",
      "\n",
      "tensor([0.5184], grad_fn=<MulBackward0>): \n",
      "This value indicates that if W1[1] is increased by a small amount,\n",
      "the loss L is expected to also increase (since the gradient is positive),\n",
      "suggesting the model would be getting worse.\n"
     ]
    }
   ],
   "source": [
    "# Values for illustration\n",
    "print(loss)\n",
    "print(a2)\n",
    "print(dL_da2)\n",
    "print(y)\n",
    "print()\n",
    "print(f\"\"\"{dL_dw1_2}: \n",
    "This value indicates that if W1[1] is increased by a small amount,\n",
    "the loss L is expected to also increase (since the gradient is positive),\n",
    "suggesting the model would be getting worse.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06adba0",
   "metadata": {},
   "source": [
    "#### To understand the effect of b1[0]= 0.1 on the loss, we perform the backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "15b4afec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1$"
      ],
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivative of Output (Z11) w.r.t. b11\n",
    "# z2 = a2\n",
    "x1_, w11_, b11_ = sp.symbols('x1 w11 b11')\n",
    "\n",
    "# Define the function\n",
    "z11_ = (x1_ * w11_) + b11_\n",
    "\n",
    "# Compute the derivative with respect to y\n",
    "derivative_f = sp.diff(z11_, b11_)\n",
    "derivative_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40ba0f",
   "metadata": {},
   "source": [
    "**NOTE**: For the bias, its direct influence on any output is through addition, so the derivative of the output with respect to the bias is =1 since if you increase the bias by a tiny amount, the output increases by the same amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fb93f",
   "metadata": {},
   "source": [
    "The partial derivative of f = ab + c with respect to c is simply 1. This indicates that for any change in c, f changes by the same amount, independent of the values of a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f8690930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass calculations for b1[0]\n",
    "# dL/da2\n",
    "dL_da2 = a2 - y # output - target\n",
    "\n",
    "# Since a2 = Z2 directly\n",
    "# represents the derivative of the output Z2 w.r.t the activation A11 of the first neuron in the hidden layer.\n",
    "dZ2_da1_1 = W2[0] \n",
    "\n",
    "# derivative of the ReLU activation function applied to the pre-activation Z1[0] of the first neuron.\n",
    "da1_1_d_z1_1 = F.relu(Z1[0])\n",
    "\n",
    "# derivative of the pre-activation Z1[0] with respect to the bias b11\n",
    "z1_1_d_b1_1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0eb254d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule to get dL/db1_1\n",
    "dL_db1_1 = dL_da2 * dZ2_da1_1 * da1_1_d_z1_1 * z1_1_d_b1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "293446c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2592], grad_fn=<MulBackward0>)\n",
      "tensor([1.4200], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([0.2880], grad_fn=<MulBackward0>): \n",
      "This value indicates that if b1[0] is increased by a small amount,\n",
      "the loss L is expected to also increase (since the gradient is positive),\n",
      "suggesting the model would be getting worse.\n"
     ]
    }
   ],
   "source": [
    "# Values for illustration\n",
    "print(loss)\n",
    "print(a2)\n",
    "print()\n",
    "print(f\"\"\"{dL_db1_1}: \n",
    "This value indicates that if b1[0] is increased by a small amount,\n",
    "the loss L is expected to also increase (since the gradient is positive),\n",
    "suggesting the model would be getting worse.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9a252665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights1: tensor([[0.0964, 0.1957],\n",
      "        [0.2928, 0.3914]], requires_grad=True)\n",
      "Updated biases1: tensor([0.0964, 0.1957], requires_grad=True)\n",
      "Updated weights2: tensor([[0.4942],\n",
      "        [0.5914]], requires_grad=True)\n",
      "Updated biases2: tensor([0.2928], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "loss.backward()  # PyTorch computes all gradients automatically\n",
    "\n",
    "# Manually updating the parameters (weights and biases), normally we'd do this with an optimizer\n",
    "learning_rate = 0.01\n",
    "with torch.no_grad():  # Updates should not be part of the computational graph\n",
    "    W1 -= learning_rate * W1.grad\n",
    "    b1 -= learning_rate * b1.grad\n",
    "    W2 -= learning_rate * W2.grad\n",
    "    b2 -= learning_rate * b2.grad\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "#     W1.grad.zero_()\n",
    "#     b1.grad.zero_()\n",
    "#     W2.grad.zero_()\n",
    "#     b2.grad.zero_()\n",
    "\n",
    "# Print updated parameters for verification\n",
    "print(f\"Updated weights1: {W1}\")\n",
    "print(f\"Updated biases1: {b1}\")\n",
    "print(f\"Updated weights2: {W2}\")\n",
    "print(f\"Updated biases2: {b2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a3010663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.4000]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.3600, 0.4320],\n",
      "        [0.7200, 0.8640]])\n",
      "tensor([[0.0964, 0.1957],\n",
      "        [0.2928, 0.3914]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# W1[0][1]_bef = 0.12\n",
    "print(W1_bef)\n",
    "print(W1.grad)\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9395246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.6000]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.5760],\n",
      "        [0.8640]])\n",
      "tensor([[0.4942],\n",
      "        [0.5914]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# W2_bef[0] = 0.5\n",
    "print(W2_bef)\n",
    "print(W2.grad)\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "13be98cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000, grad_fn=<SelectBackward0>)\n",
      "tensor([0.3600, 0.4320])\n",
      "tensor([0.0964, 0.1957], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# b1_bef[0] = 0.1\n",
    "print(b1_bef[0])\n",
    "print(b1.grad)\n",
    "print(b1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
