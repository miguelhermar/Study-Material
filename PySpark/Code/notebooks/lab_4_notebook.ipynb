{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Query Plans Lab\n",
    "> Download the dataset from [the official TLC Trip Record Data website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell only shows how to document code\n",
    "```python\n",
    "# Load file\n",
    "local_file = 'datasets/your-downloaded-from-TLC-taxis-file-here.parquet'\n",
    "\n",
    "# Show data\n",
    "spark.read.parquet(local_file).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col, year"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is master(local N)?\n",
    "The --master option specifies the master URL for a distributed cluster, or local to run locally with one thread, or local[N] to run locally with N threads.\n",
    "\n",
    "<b>Source</b>: See Spark [docs here](spark.apache.org/docs/latest). See all [options here](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "             .master(\"local[1]\")\\\n",
    "             .appName(\"spark-app-version-x\")\\\n",
    "             .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read non-partitioned taxi data\n",
    "local_files = 'datasets/parquet/'\n",
    "df_taxis_non_partitioned_raw = spark.read.parquet(local_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we cleaned the data in the previous notebook, let's do the same:\n",
    "df_taxis_non_partitioned_raw = df_taxis_non_partitioned_raw.where(year(col('tpep_pickup_datetime')) == '2023')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Make sure you execute the following, from the previous notebook, to generate the partitioned data:\n",
    "\n",
    "```python\n",
    "    df_sink = df_clean_s1.withColumn(\"p_date\",to_date(col('tpep_pickup_datetime')))\n",
    "    df_sink.write.partitionBy(\"p_date\").mode(\"append\").parquet(\"datasets/yellow_taxis_daily/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned taxi data\n",
    "local_path = 'datasets/yellow_taxis_daily/'\n",
    "df_taxis_daily_raw = spark.read.parquet(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show schema and find new partition column\n",
    "df_taxis_daily_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show new partition column\n",
    "df_taxis_daily_raw.select('p_date').show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create same column p_date, so we can compare plans\n",
    "df_taxis_nopartitioned_raw = df_taxis_non_partitioned_raw.withColumn(\"p_date\",to_date(col('tpep_pickup_datetime')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Non-partitioned DF as View\n",
    "df_taxis_nopartitioned_raw.createOrReplaceTempView(\"tbl_taxis_nopartitioned_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Daily DF as View\n",
    "df_taxis_daily_raw.createOrReplaceTempView(\"tbl_taxis_daily_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q1a = spark.sql(\"select avg(trip_distance) from tbl_taxis_daily_raw where p_date='2023-02-14' and RatecodeID=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "q1a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain plan\n",
    "q1a.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q1b = spark.sql(\"select avg(trip_distance) from tbl_taxis_nopartitioned_raw where p_date='2023-02-14' and RatecodeID=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain plan\n",
    "q1b.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q2a = spark.sql(\"select p_date,count(1) from tbl_taxis_daily_raw where p_date in ('2023-02-14','2023-02-15','2023-02-16') group by p_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "q2b = spark.sql(\"select p_date,count(1) from tbl_taxis_nopartitioned_raw where p_date in ('2023-02-14','2023-02-15','2023-02-16') group by p_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plan\n",
    "q2a.explain(extended=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plan\n",
    "q2b.explain(extended=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plan\n",
    "q2a.explain(extended=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plan\n",
    "q2b.explain(extended=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "spark.sql(\"select p_date,count(1) from tbl_taxis_daily_raw group by p_date order by to_date(p_date)\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# Query by partition Key; i.e. using '2023-02-14' as filter\n",
    "spark.sql(\"select p_date,count(1) from tbl_taxis_nopartitioned_raw group by p_date order by to_date(p_date)\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
